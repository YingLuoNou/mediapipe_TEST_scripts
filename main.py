import cv2
import time
import mediapipe as mp
import os
import urllib.request
import sys
import threading
import queue

# 1. ç¡¬ç¼–ç  33 ä¸ªå…³é”®ç‚¹çš„è¿æ¥å…³ç³»
POSE_CONNECTIONS = [
    (0, 1), (1, 2), (2, 3), (3, 7),
    (0, 4), (4, 5), (5, 6), (6, 8),
    (9, 10),
    (11, 12),
    (11, 13), (13, 15), (15, 17), (15, 19), (15, 21), (17, 19),
    (12, 14), (14, 16), (16, 18), (16, 20), (16, 22), (18, 20),
    (11, 23), (12, 24), (23, 24),
    (23, 25), (25, 27), (27, 29), (27, 31), (29, 31),
    (24, 26), (26, 28), (28, 30), (28, 32), (30, 32)
]

# ================= è¾…åŠ©æ¨¡å—ï¼šæ¨¡å‹ä¸‹è½½ç®¡ç† =================
MODELS_INFO = {
    "lite": ("pose_landmarker_lite.task", "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task"),
    "full": ("pose_landmarker_full.task", "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task"),
    "heavy": ("pose_landmarker_heavy.task", "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task")
}

def dl_progress_hook(count, block_size, total_size):
    """æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡"""
    percent = int(count * block_size * 100 / total_size)
    if percent <= 100:
        sys.stdout.write(f"\rä¸‹è½½è¿›åº¦: {percent}%")
        sys.stdout.flush()

def check_models():
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹ï¼Œè¿”å›å¯ç”¨å’Œç¼ºå¤±çš„æ¨¡å‹åˆ—è¡¨"""
    available, missing = [], []
    for key, (filename, _) in MODELS_INFO.items():
        if os.path.exists(filename):
            available.append(key)
        else:
            missing.append(key)
    return available, missing

# ================= è¾…åŠ©æ¨¡å—ï¼šå¤šçº¿ç¨‹è§†é¢‘æµæå– =================
class VideoStream:
    """åŒ…è£…äº†åŸç”Ÿ VideoCaptureï¼Œæ”¯æŒå¼€å¯å¤šçº¿ç¨‹é˜Ÿåˆ—è¯»å–ä»¥æå‡å¸§ç‡"""
    def __init__(self, src, is_camera=False, use_threading=False):
        self.cap = cv2.VideoCapture(src)
        self.is_opened = self.cap.isOpened()
        self.use_threading = use_threading
        self.is_camera = is_camera
        
        if not self.is_opened:
            return

        if self.is_camera:
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 10000)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 10000)
            actual_w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            actual_h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            print(f"[æ‘„åƒå¤´] å·²è‡ªåŠ¨åå•†è‡³æœ€é«˜åˆ†è¾¨ç‡: {actual_w} x {actual_h}")

        if self.use_threading:
            self.q = queue.Queue(maxsize=15) 
            self.stopped = False
            self.thread = threading.Thread(target=self._update, daemon=True)
            self.thread.start()

    def _update(self):
        """åå°çº¿ç¨‹ï¼šä¸æ–­è¯»å–è§†é¢‘å¸§æ”¾å…¥é˜Ÿåˆ—"""
        while not self.stopped:
            ret, frame = self.cap.read()
            if not ret:
                self.stopped = True
                return
            
            if self.is_camera:
                # å®æ—¶æ‘„åƒå¤´æ¨¡å¼ä¸‹çš„â€œé›¶å»¶è¿Ÿâ€ç­–ç•¥ï¼šæ¸…ç©ºç§¯å‹ï¼Œåªä¿ç•™æœ€æ–°å¸§
                while not self.q.empty():
                    try:
                        self.q.get_nowait()
                    except queue.Empty:
                        break
                self.q.put((ret, frame))
            else:
                # æœ¬åœ°è§†é¢‘æ¨¡å¼ä¸‹çš„â€œä¸ä¸¢å¸§â€ç­–ç•¥
                if not self.q.full():
                    self.q.put((ret, frame))
                else:
                    time.sleep(0.005)

    def read(self):
        """è¯»å–ä¸€å¸§ç”»é¢"""
        if self.use_threading:
            if self.stopped and self.q.empty():
                return False, None
            while self.q.empty():
                if self.stopped:
                    return False, None
                time.sleep(0.001)
            return self.q.get()
        else:
            return self.cap.read()

    def release(self):
        if self.use_threading:
            self.stopped = True
            if hasattr(self, 'thread'):
                self.thread.join()
        else:
            self.cap.release()

    def get(self, prop_id):
        return self.cap.get(prop_id)

# ================= æ ¸å¿ƒå¤„ç†æ¨¡å— =================
def show_fit_window(window_name, image, max_width=1280, max_height=720):
    h, w = image.shape[:2]
    if w > max_width or h > max_height:
        scale = min(max_width / w, max_height / h)
        display_image = cv2.resize(image, (int(w * scale), int(h * scale)))
    else:
        display_image = image
    cv2.imshow(window_name, display_image)

def draw_landmarks_on_image(rgb_image, detection_result):
    annotated_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
    if not detection_result.pose_landmarks:
        return annotated_image

    h, w, _ = annotated_image.shape
    for pose_landmarks in detection_result.pose_landmarks:
        keypoints = []
        for landmark in pose_landmarks:
            cx, cy = int(landmark.x * w), int(landmark.y * h)
            keypoints.append((cx, cy))
            cv2.circle(annotated_image, (cx, cy), 4, (24, 200, 150), -1)

        for connection in POSE_CONNECTIONS:
            start_idx, end_idx = connection
            if start_idx < len(keypoints) and end_idx < len(keypoints):
                pt1 = keypoints[start_idx]
                pt2 = keypoints[end_idx]
                cv2.line(annotated_image, pt1, pt2, (250, 100, 10), 2)
                
    return annotated_image

def process_image(model_path, image_path, save_output=False, model_name=""):
    if not os.path.exists(image_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶ {image_path}")
        return

    BaseOptions = mp.tasks.BaseOptions
    PoseLandmarker = mp.tasks.vision.PoseLandmarker
    options = mp.tasks.vision.PoseLandmarkerOptions(
        base_options=BaseOptions(model_asset_path=model_path),
        running_mode=mp.tasks.vision.RunningMode.IMAGE,
        num_poses=1
    )

    with PoseLandmarker.create_from_options(options) as landmarker:
        cv_mat = cv2.imread(image_path)
        rgb_frame = cv2.cvtColor(cv_mat, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

        detection_result = landmarker.detect(mp_image)
        result_img = draw_landmarks_on_image(rgb_frame, detection_result)
        
        if save_output:
            base, ext = os.path.splitext(image_path)
            suffix = f"_{model_name}" if model_name else ""
            out_path = f"{base}{suffix}_output{ext}"
            cv2.imwrite(out_path, result_img)
            print(f"å·²ä¿å­˜è‡³: {out_path}")

        show_fit_window('MediaPipe Pose Estimation', result_img)
        print("å¤„ç†å®Œæˆã€‚æŒ‰ä»»æ„é”®é€€å‡º...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()

def process_video_stream(model_path, source, is_camera=False, save_output=False, use_threading=False, headless=False, model_name=""):
    BaseOptions = mp.tasks.BaseOptions
    PoseLandmarker = mp.tasks.vision.PoseLandmarker
    options = mp.tasks.vision.PoseLandmarkerOptions(
        base_options=BaseOptions(model_asset_path=model_path),
        running_mode=mp.tasks.vision.RunningMode.VIDEO,
        num_poses=1
    )

    stream = VideoStream(source, is_camera=is_camera, use_threading=use_threading)
    if not stream.is_opened:
        print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘æº {source}")
        return

    out_writer = None
    if save_output:
        suffix = f"_{model_name}" if model_name else ""
        if is_camera:
            out_path = f"camera{suffix}_output_{int(time.time())}.mp4"
            fps = 30.0 
        else:
            base, ext = os.path.splitext(str(source))
            out_path = f"{base}{suffix}_output.mp4"
            fps = stream.get(cv2.CAP_PROP_FPS)
            if fps == 0 or fps != fps:
                fps = 30.0
        print(f"å‡†å¤‡ä¿å­˜è§†é¢‘è‡³: {out_path}")

    start_time = time.monotonic()
    prev_frame_time = start_time
    last_timestamp_ms = -1
    frame_count = 0

    print("\n" + "="*30)
    if headless:
        print("ğŸš€ å·²å¯åŠ¨ã€æ— å¤´æ¨¡å¼ (æé™å¸§ç‡)ã€‘ï¼Œå°†ä¸ä¼šæ˜¾ç¤ºç”»é¢ã€‚")
        print("ğŸ’¡ æç¤º: éšæ—¶æŒ‰ Ctrl + C ç»“æŸå¹¶ä¿å­˜ï¼")
    else:
        print("ğŸš€ æ­£åœ¨è¿è¡Œï¼ŒæŒ‰ 'q' é”®é€€å‡ºã€‚")
    print("="*30 + "\n")

    with PoseLandmarker.create_from_options(options) as landmarker:
        try:
            while True:
                success, frame = stream.read()
                if not success:
                    if not is_camera:
                        print("\nè§†é¢‘æµå¤„ç†ç»“æŸã€‚")
                    break

                if is_camera:
                    frame = cv2.flip(frame, 1)

                if save_output and out_writer is None:
                    h, w = frame.shape[:2]
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
                    out_writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))

                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
                
                timestamp_ms = int((time.monotonic() - start_time) * 1000)
                if timestamp_ms <= last_timestamp_ms:
                    timestamp_ms = last_timestamp_ms + 1
                last_timestamp_ms = timestamp_ms

                detection_result = landmarker.detect_for_video(mp_image, timestamp_ms)
                result_frame = draw_landmarks_on_image(rgb_frame, detection_result)

                if save_output and out_writer is not None:
                    out_writer.write(result_frame)

                new_frame_time = time.monotonic()
                current_fps = 1 / (new_frame_time - prev_frame_time) if (new_frame_time - prev_frame_time) > 0 else 0
                prev_frame_time = new_frame_time
                frame_count += 1

                # === æ˜¾ç¤ºä¸åˆ·æ–°é€»è¾‘ ===
                if headless:
                    if frame_count % 30 == 0:
                        print(f"æ­£åœ¨åå°æé€Ÿå¤„ç†... å½“å‰å¸§ç‡: {int(current_fps)} FPS")
                else:
                    cv2.putText(result_frame, f'FPS: {int(current_fps)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    show_fit_window('MediaPipe Pose Estimation', result_frame)
                    delay = 1 
                    if cv2.waitKey(delay) & 0xFF == ord('q'):
                        break

        except KeyboardInterrupt:
            print("\n[ä¸­æ–­] æ¥æ”¶åˆ° Ctrl+Cï¼Œæ­£åœ¨ç»ˆæ­¢ç¨‹åº...")

    # --- æ–°å¢ï¼šæ ¸å¿ƒç»Ÿè®¡æ•°æ®è¾“å‡º ---
    total_time = time.monotonic() - start_time
    if frame_count > 0 and total_time > 0:
        avg_fps = frame_count / total_time
        print("\n" + "="*35)
        print("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        print("="*35)
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name.capitalize()}")
        print(f"æ€»å¤„ç†å¸§æ•°: {frame_count} å¸§")
        print(f"æ€»è€—æ—¶:   {total_time:.2f} ç§’")
        print(f"å¹³å‡å¸§ç‡:   {avg_fps:.2f} FPS")
        print("="*35 + "\n")

    stream.release()
    if out_writer is not None:
        out_writer.release()
        print(f"âœ… è§†é¢‘å·²æˆåŠŸä¿å­˜è‡³: {out_path}")
    cv2.destroyAllWindows()

def main():
    while True:
        available_models, missing_models = check_models()
        
        print("\n" + "="*45)
        print("ğŸ¤– MediaPipe å§¿åŠ¿æ£€æµ‹ç»¼åˆå·¥å…· (é«˜æ€§èƒ½ç‰ˆ)")
        print("="*45)
        
        if missing_models:
            print("0. â¬‡ï¸  ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹ (Lite/Full/Heavy)")
        print("1. ğŸ–¼ï¸  æµ‹è¯•å•å¼ å›¾ç‰‡")
        print("2. ğŸ¬  æµ‹è¯•æœ¬åœ°è§†é¢‘æ–‡ä»¶")
        print("3. ğŸ“·  ä½¿ç”¨ USB æ‘„åƒå¤´å®æ—¶æ•æ‰")
        print("q. âŒ  é€€å‡ºç¨‹åº")
        print("="*45)
        
        choice = input("è¯·è¾“å…¥é€‰é¡¹: ").strip().lower()
        
        if choice == 'q':
            print("å†è§ï¼")
            break
            
        # =============== ä¸‹è½½æ¨¡å‹é€»è¾‘ ===============
        if choice == '0' and missing_models:
            print("\nå‘ç°ä»¥ä¸‹ç¼ºå¤±æ¨¡å‹ï¼š")
            for i, m in enumerate(missing_models):
                print(f"{i+1}. {m.capitalize()} æ¨¡å‹")
            print("a. å…¨éƒ¨ä¸‹è½½")
            
            dl_choice = input("\nè¯·é€‰æ‹©è¦ä¸‹è½½çš„é¡¹ (ä¾‹å¦‚è¾“å…¥ 1 æˆ– a): ").strip().lower()
            to_download = []
            
            if dl_choice == 'a':
                to_download = missing_models
            elif dl_choice.isdigit() and 1 <= int(dl_choice) <= len(missing_models):
                to_download.append(missing_models[int(dl_choice)-1])
            else:
                print("æ— æ•ˆè¾“å…¥ï¼")
                continue
                
            for m in to_download:
                filename, url = MODELS_INFO[m]
                print(f"\nå¼€å§‹ä¸‹è½½ {filename} ...")
                try:
                    urllib.request.urlretrieve(url, filename, dl_progress_hook)
                    print(f"\nâœ… {filename} ä¸‹è½½æˆåŠŸï¼")
                except Exception as e:
                    print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
            continue

        # =============== è¿è¡Œæ£€æµ‹é€»è¾‘ ===============
        if choice in ['1', '2', '3']:
            if not available_models:
                print("\nâš ï¸ æœ¬åœ°æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼è¯·å…ˆè¾“å…¥ 0 è¿›è¡Œä¸‹è½½ã€‚")
                continue
                
            # é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹
            print("\nè¯·é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹ç²¾åº¦ï¼š")
            for i, m in enumerate(available_models):
                print(f"{i+1}. {m.capitalize()}")
            m_choice = input("è¯·è¾“å…¥åºå·: ").strip()
            
            if not (m_choice.isdigit() and 1 <= int(m_choice) <= len(available_models)):
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¿”å›ä¸»èœå•ã€‚")
                continue
                
            selected_model = available_models[int(m_choice)-1] 
            model_path = MODELS_INFO[selected_model][0]
            
            # å•å¼ å›¾ç‰‡å¤„ç†
            if choice == '1':
                img_path = input("\nè¯·è¾“å…¥å›¾ç‰‡è·¯å¾„: ").strip().strip('"').strip("'")
                save_choice = input("æ˜¯å¦ä¿å­˜ç…§ç‰‡ï¼Ÿ(y/n) [é»˜è®¤ n]: ").strip().lower() == 'y'
                process_image(model_path, img_path, save_output=save_choice, model_name=selected_model)
                
            # è§†é¢‘ä¸æ‘„åƒå¤´å¤„ç†
            elif choice in ['2', '3']:
                if choice == '3':
                    src_input = input("\nè¯·è¾“å…¥æ‘„åƒå¤´è®¾å¤‡å· (ç›´æ¥å›è½¦é»˜è®¤ä¸º 0): ").strip()
                    src = int(src_input) if src_input.isdigit() else 0
                    is_camera = True
                else:
                    src = input("\nè¯·è¾“å…¥è§†é¢‘è·¯å¾„: ").strip().strip('"').strip("'")
                    is_camera = False
                    
                save_choice = input("æ˜¯å¦ä¿å­˜è§†é¢‘ç»“æœï¼Ÿ(y/n) [é»˜è®¤ n]: ").strip().lower() == 'y'
                
                print("\nè¯·é€‰æ‹©æ€§èƒ½æ¨¡å¼ï¼š")
                print("1. æ ‡å‡†æ¨¡å¼ (å•çº¿ç¨‹ä¸²è¡Œ + UIæ˜¾ç¤º)")
                print("2. æ— å¤´æ¨¡å¼ (å•çº¿ç¨‹ä¸²è¡Œ + éšè—UIæé™è¿ç®—)")
                print("3. å¤šçº¿ç¨‹æ¨¡å¼ (ç‹¬ç«‹çº¿ç¨‹è¯»å–ç¼“å­˜ + UIæ˜¾ç¤º)")
                print("4. æ€§èƒ½æ€ªå…½æ¨¡å¼ (ç‹¬ç«‹çº¿ç¨‹è¯»å–ç¼“å­˜ + éšè—UIæé™è¿ç®—)")
                mode_choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2/3/4) [é»˜è®¤ 1]: ").strip()
                
                headless = mode_choice in ['2', '4']
                use_threading = mode_choice in ['3', '4']
                
                process_video_stream(model_path, src, is_camera, save_choice, use_threading, headless, model_name=selected_model)

if __name__ == '__main__':
    main()